1
00:00:08,931 --> 00:00:22,058
データ前処理を経てベクターデータベースの構築方法を見ました。次は、RAGシステムでの活用法を見ましょう。

2
00:00:24,140 --> 00:00:38,427
検索段階では、ユーザーの質問もエンベディングされ、エンベディングされたユーザーの質問とベクターデータベースに含まれる文書との間で類似度を計算し、最も類似した文書を抽出します。

3
00:00:38,468 --> 00:00:40,490
この検索の過程も含まれます。

4
00:00:43,392 --> 00:00:57,043
先ほど説明した検索の過程、つまりエンベディングされたユーザーの質問とベクターデータベース内の文書断片の内容が意味的に類似しているかを見つけ出す検索を、セマンティックサーチと呼びます。

5
00:00:58,685 --> 00:01:13,713
単にキーワードの一致で探すキーワード検索とは異なり、テキスト、画像、オーディオなどを多次元空間上に配置し、その距離の近さによって意味的な類似性を判断できるようになります。

6
00:01:17,334 --> 00:01:26,337
意味的に類似しているか、距離が近いかを判断するために、前回の時間でエンベディングを通じてベクトルに変換したのです。

7
00:01:27,557 --> 00:01:31,237
その距離は一般的にコサイン類似度を使用します。

8
00:01:32,438 --> 00:01:44,445
コサイン類似度はベクトルの長さだけでなく、ベクトルの方向も一緒に考慮するため、二つのベクトルがどれだけ同じ方向を向いているかを測定する指標として利用できます。

9
00:01:49,501 --> 00:01:55,200
このようなリトリーバーの中で、最も基本的なのがベクターストア・リトリーバーです。

10
00:01:57,474 --> 00:02:06,160
ベクターストア・リトリーバーは、ベクターストア自体を検索エンジンとして使用し、ベクターストアから類似した文書を見つけ出す役割を果たします。

11
00:02:07,156 --> 00:02:16,401
ここには三つの検索方式があり、まずSimilarityは類似したK件の文書を返す方式です。

12
00:02:17,723 --> 00:02:36,735
Top Kを指定することで返す文書の数、検索する文書の数を設定でき、二つ目のSimilarity Score Thresholdは、類似度スコアにしきい値を設けて、一定基準以上の文書だけを検索する機能です。

13
00:02:36,775 --> 00:02:37,856
という機能です。

14
00:02:38,858 --> 00:02:45,522
関連性の低い文書をフィルタリングし、類似度が高い文書だけを選別することができます。

15
00:02:48,564 --> 00:02:59,192
検索品質を高めるのには有用ですが、この数値自体をあまりにも高く設定すると、まったく文書が検索されない場合も発生することがあります。

16
00:03:01,834 --> 00:03:09,474
最後に、mmrは類似しつつも互いに異なる情報を含む文書を検索する方式です。

17
00:03:10,854 --> 00:03:22,798
一般的に、類似した文書だけを抽出すると、その内容も似ている可能性が高いため、あまりにも似通った内容を最小限に抑え、情報の多様性を高めるための方法です。

18
00:03:25,019 --> 00:03:29,420
より多様な情報を含める必要がある場合に有用に活用できます。

19
00:03:31,856 --> 00:03:53,462
従来のリトリーバーは、ユーザーの質問、つまりクエリをそのままベクトルに変換して検索を行っていました。 一つ例を挙げると、私が「RAGについて知りたい」と質問したと仮定した場合、従来のリトリーバーでは「RAGについて知りたい」という内容を入力として、類似した文書を検索することになります。

20
00:03:55,182 --> 00:04:12,570
しかし、この質問をいくつかに分けてみると、RAGの定義、RAGの構成要素、RAGの限界という三つに分けて検索することができます。このように分けて検索することで、より正確で具体的な情報を得ることができるでしょう。

21
00:04:14,032 --> 00:04:18,293
このように検索する方法がマルチクエリーリトリーバーです。

22
00:04:20,275 --> 00:04:31,394
一つの質問を複数の意味のあるクエリに細分化した後、それぞれのクエリに基づいて個別に検索することで、より多様な文書を検索することができます。

23
00:04:33,495 --> 00:04:47,521
このようにすると、特定のクエリだけに依存して検索されるのではなく、さまざまな文書から多くの情報を取り入れて、より正確で有用な回答を得ることができるようになります。

24
00:04:50,084 --> 00:05:26,274
最後にご紹介するのは、Long Context Reorderという手法です。 この方法について説明する前に、NeedleやHeist、stakeという問題について見ておく必要があります。 この資料は、ジーピーティーフォー いちにっぱちけー モデルが長い文書からどれだけ正確に情報を見つけ出せるかを実験した結果です。トークンの長さが長くなるほど、エルエルエムが全トークンの中から必要な情報、つまり本当に必要な情報を見つけ出す検索精度が低下するという結果を示しています。

25
00:05:28,374 --> 00:05:45,319
一番上や一番下に情報がある場合は必要な部分をうまく見つけ出せますが、文書の中間のどこかにある場合には、検索精度や回答品質の精度が急激に低下することが確認できます。

26
00:05:46,719 --> 00:06:00,183
文書が長くなるほど、その位置によってLLMが情報を見つけられるかどうかが変わるため、情報をどこに配置し、どのように並べるかも重要です。

27
00:06:01,375 --> 00:06:21,729
長い文書から情報を探し出すには限界があり、その限界を克服するために登場した手法がLong Context Reorderです。 これはコンテキスト内で重要な情報を再配置する方法であり、これによってLLMが長いコンテキスト内でも意味のある情報をしっかりと捉えられるよう最適化できます。

28
00:06:23,170 --> 00:06:28,753
大規模なデータを扱う際には、必ず考慮すべきポイントだと言えます。

29
00:06:31,215 --> 00:06:34,496
まずはVectorStore Retrieverから見ていきましょう。

30
00:06:35,577 --> 00:06:41,499
VectorStore Retrieverは、VectorStoreを使って文書を検索するリトリーバーです。

31
00:06:43,340 --> 00:06:53,444
VectorStoreに実装されているSimilarity SearchやMMRなどの検索メソッドを利用してテキストを検索します。

32
00:06:56,726 --> 00:06:59,407
ここではFaceを使用します。

33
00:07:03,283 --> 00:07:18,987
実習で使うファイルはappendix keywordというテキストファイルで、このテキストファイルを前にやったようにロードし、テキストを分割し、エンベディングしてベクターデータベースに入れてみます。

34
00:07:20,068 --> 00:07:22,668
必要な環境変数も読み込んだ後に

35
00:07:25,429 --> 00:07:37,215
appendix keywordというテキストファイルをテキストローダーで読み込み、キャラクターテキストスプリッターを使ってチャンクサイズ300、オーバーラップなしでテキストを分割します。

36
00:07:40,417 --> 00:07:44,461
エンベディングモデルは同じく3エルモデルを使用します。

37
00:07:46,383 --> 00:07:49,685
このように分割されたチャンクをfaceに

38
00:07:52,348 --> 00:07:53,008
入れます。

39
00:07:54,970 --> 00:08:11,519
前の段階でdbという名前のface vector dbが作成され、asRetrieverメソッドを通じてこの作成されたDBを検索エンジンとして使えるようにします。

40
00:08:13,119 --> 00:08:24,524
ここでユーザーが質問を入力すると、その質問がエンベディングされ、エンベディングされた後にFaceVectorDbから類似したドキュメントが検索されます。

41
00:08:25,985 --> 00:08:30,106
そして検索を実行する際にはRetrieverInvokeメソッドを使用します。

42
00:08:31,543 --> 00:09:14,463
これはリトリーバーの主なエントリーポイントで、関連する文書を検索し、類似した文書を探すのに使われます。「インベディングとは何ですか？」というクエリを入力して、このベクターデータベース内で検索を実行してみます。レポート数は4だったので、4つの文書が検索されたことが確認でき、また「インベディングとは何ですか？」という質問に対して、インベディングの定義やWordににVecの定義、セマンティックサーチなどの内容が検索されたことが分かります。

43
00:09:15,743 --> 00:09:32,881
パラメータとしては、先ほど理論の時間で説明したSimilarityやMMRのようなタイプを設定することができ、返却する文書の数や閾値（スレッショルド）も設定することができます。

44
00:09:33,722 --> 00:09:47,732
Top Kを1に変更して検索してみると、1件だけが検索されたことを確認できますし、Thresholdを0.5に設定して、類似度が0.5以上の文書だけを検索することもできます。

45
00:09:49,533 --> 00:10:03,233
これは、クエリから検索された内容の重複を避ける方法の一つであり、文書の関連性とすでに選択された検索文書の差別性を同時に考慮する方法です。

46
00:10:04,274 --> 00:10:17,665
Search TypeをMMRに設定して検索を進めることができ、Search TypeはMMRで2件だけ検索し、Fetch KやLambda Vaultまで設定します。

47
00:10:18,165 --> 00:10:32,299
Fetch Kの場合は、MMRに最初に渡す文書の数であり、例えば10件を渡して、その後フィルタリングされて最終的に2件を返すという意味で捉えることができます。

48
00:10:32,859 --> 00:10:38,142
ラムダはMML結果の多様性をどの程度にするかの閾値です。

49
00:10:39,363 --> 00:10:47,006
このように設定したRetrieverに「インベディングとは何ですか？」という同じ質問を入力して検索を行ってみます。

50
00:10:48,367 --> 00:10:53,189
これなら、次のように2つの文書が検索されたことを確認できます。

51
00:10:53,925 --> 00:11:11,456
次はマルチクエリーリトリーバーです。 マルチクエリーリトリーバーは、入力クエリーを複数のクエリーに自動的に生成し、その生成された複数のクエリーがそれぞれ検索を行い、回答を作成できるようにする方式です。

52
00:11:12,697 --> 00:11:28,484
この例でも同様に、FAISSのベクターデータベースを使用する予定で、環境変数も設定した後、今回はテキストファイルではなく、最初の回で使用したDeepSeekに関連するPDFファイルを使用します。

53
00:11:29,845 --> 00:11:42,376
サーチャブルPDFなので、PyPDFローダーを使ってテキストを抽出し、その後、RecursiveTextSplitterで抽出したテキストを分割します。

54
00:11:42,817 --> 00:11:47,601
チャンクサイズは500、重なりは全くない状態で分割します。

55
00:11:48,189 --> 00:11:57,853
その後、エンベディングモデルを設定し、先ほど生成したチャンクをFAISSに投入してFAISSベクターデータベースを作成します。

56
00:11:58,773 --> 00:12:11,359
作成したFAISSベクターデータベースをリトリーバーとして使用し、このとき例として使うクエリは、DeepSeekの登場がマイクロソフトに与えた影響について検索してみます。

57
00:12:12,359 --> 00:12:15,060
同様に4つの文書が検索されました。

58
00:12:15,560 --> 00:12:25,220
1つを検索してみます... 検索された4つのうち1つを出力してみると、次のように検索されたことが確認できます。

59
00:12:26,901 --> 00:12:37,610
Multi Query Retrieverでマルチクエリを作成する際にLLMを使用することができ、私たちはGPT-4oのミニモデルを使ってみます。

60
00:12:38,931 --> 00:12:52,383
Multi Query Retrieverで使用するLLMを指定し、このLLMは追加のクエリを生成する際に使われ、その後の作業はRetrieverが残りの処理を行います。

61
00:12:53,423 --> 00:13:12,799
以下のプロセスは、マルチクエリを生成する過程を確認するために実行するコードであり、上記と同様に『딥시크の登場がマイクロソフトに与えた影響』というクエリをRetrieverを使って検索してみます。

62
00:13:14,701 --> 00:13:33,930
そのときのドキュメントの数は9件です。 このロギングに出力された部分を見ると、私たちが入力として入れた『딥시크の登場がマイクロソフトに与えた影響』というクエリが、このように3つのサブクエリに分割されたことが確認できます。

63
00:13:34,630 --> 00:13:52,938
どのような影響を与えたのか、どのように評価できるのか、どんな変化や影響を受けたのか、このような観点でもう少し詳しく見ていきましょう。 具体的な内容でクエリが生成され、これらのクエリがそれぞれ検索され、最終的に回答を生成する際に使用されます。

64
00:13:53,439 --> 00:14:04,888
もう一つの方法としては、LCLチェーンを活用する方法がありますが、LCL文法については前の講義でもご確認いただいたと思いますので、簡単に触れるだけにします。

65
00:14:05,369 --> 00:14:19,118
ユーザーからの入力があった場合、この入力はプロンプトテンプレートに渡され、作成されたプロンプトはLLMの入力として使用され、LLMの出力はパーサーの入力として使用されます。

66
00:14:20,318 --> 00:14:31,024
まずプロンプトテンプレートを定義する必要がありますが、前回とは異なり、5つのサブクエリを生成できるようにプロンプトを作成してみます。

67
00:14:32,145 --> 00:14:41,231
その後、チェーンを構成し、そのチェーンにユーザーの質問を一度渡してみたいと思います。

68
00:14:43,833 --> 00:14:56,365
出てきた結果を確認すると、以前は3つのサブクエリに分かれていましたが、今回はプロンプト自体を5つのサブクエリに作成するように構成したため、

69
00:14:59,420 --> 00:15:09,903
一つの質問、元の質問が五つに分割されたことを確認できます。 このように作成したチェーンをマルチコルリトリーバーの中に渡して、リトリーバーとして使用することができます。

70
00:15:11,984 --> 00:15:27,488
結果を確認してみると、以前は3個でしたが、今はカスタムチェーンを使ったので5個のサブクエリが作成され、それぞれが検索を行い、全体で検索された文書は合計9件です。

71
00:15:28,484 --> 00:15:34,526
ということを確認できます。 最後にご紹介するのは、Long Context Reorderです。

72
00:15:36,746 --> 00:15:48,870
検索された文書の数が多くなりすぎてコンテキストが長くなると、その中間にある情報をうまく見つけられない傾向が現れます。

73
00:15:50,570 --> 00:16:02,557
この問題を解決するために、検索後に文書の順序を再配置してパフォーマンスの低下を防ぐ方法がLong Context Reorderです。

74
00:16:04,078 --> 00:16:12,080
今回はChroma Vector DBを使用します。 同様に、環境変数の設定に関するモジュールを追加します。

75
00:16:13,820 --> 00:16:29,777
モデルは同じく3エルモデルを使い、このテキスト、まずは任意の10この文をテキストリストとして受け取り、これらのテキストをChroma DBに入れてvectordbを作成してみます。

76
00:16:30,738 --> 00:16:38,485
10こすべてを検索して、最終的にこの順序が正しく並んでいるかを確認しようと思います。

77
00:16:39,465 --> 00:16:52,617
質問として「ChatGPTについて何を教えてくれますか？」という内容で、上でkを12に設定したので全体が検索されることになります。この順序を今回は確認していただければと思います。

78
00:16:54,118 --> 00:17:11,093
ChatGPTに関連する内容が一番目、二番目、三番目、四番目、五番目まであり、類似度によって並べ替えられているため、質問と関連性の高い文書が上位に位置していることが確認できます。

79
00:17:12,855 --> 00:17:17,739
次に、このように検索された文書を再度並べ替えてみます。

80
00:17:19,761 --> 00:17:22,764
コンテキストリオーダーを使って再度並べ替えてみましょう。

81
00:17:24,824 --> 00:17:39,952
もともとChatGPTに関連する文書は最初の4つの文書でしたが、リオーダーによって前に2つ、後ろに2つ、上位に配置されていることが確認できます。

