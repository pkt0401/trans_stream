1
00:00:09,587 --> 00:00:25,992
前回は抽出を学びましたが、今回はテキストを効果的に分割する方法を見ていきましょう。この分割された断片を「チャンク」と呼び、その過程を「チャンク化」と言います。

2
00:00:26,992 --> 00:00:56,646
チャンク化の過程で文書を適切な大きさに分割することで、あとの検索性能や応答の正確性、そして品質を高めることができます。 むやみに小さな単位で分割すると文脈に関する情報が不足する可能性があり、逆に大きすぎる単位で分割すると文脈は含まれるかもしれませんが、その中から重要な情報を抽出するのが難しくなることがあります。 どのように分割するかを考えたとき、最も簡単なのは同じ長さで分ける方法です。

3
00:00:57,707 --> 00:01:07,932
ただし、同じ長さで分割すると文が途中で切れて文脈がつながらなくなることがありますが、これを克服するために登場した概念がオーバーラップです。

4
00:01:09,064 --> 00:01:21,694
一定部分のチャンクを重ねて分割する方法であり、こうすることで前後のチャンク同士が重なり合い、文脈を一部維持できるという利点があります。

5
00:01:23,195 --> 00:01:39,134
これをより精巧に文書を分割するためには、次のような方法を適用することができます。 まず、セマンティック・チャンクは、単に文字数やトークン数を基準にするのではなく、文の意味的なつながりを考慮して分割する方法です。

6
00:01:40,114 --> 00:01:50,701
通常、文書では同じ話題の内容が段落という単位でまとめられていますが、このように文脈を維持できるように、ある一つの単位でまとめて処理する方法です。

7
00:01:51,982 --> 00:02:01,348
段落単位でチャンクを作成することで、自然に文脈が維持され、文同士の意味的な流れが損なわれないという利点があります。

8
00:02:02,250 --> 00:02:14,883
2番目に、メタデータベースのチャンク化は、チャンク内の情報を補強するために、ページ情報やタイトル、ひづけ、カテゴリなど、さまざまなメタデータを追加する方法です。

9
00:02:15,704 --> 00:02:23,627
検索を行う際、単にテキストだけを検索するのではなく、メタデータまで活用することで、より精密な検索が可能になります。

10
00:02:24,127 --> 00:02:31,590
例えば、論文のタイトル、著者、所属、要約などの内容をメタデータとして活用すれば、

11
00:02:33,790 --> 00:02:40,401
特定の著者の論文だけを検索するなど、検索結果をフィルタリングする際に有効に活用できます。

12
00:02:42,157 --> 00:02:42,781
それでは、

13
00:02:45,082 --> 00:02:47,584
スプリットテキストを実習してみましょう。

14
00:02:52,204 --> 00:03:01,288
今回は、テキストや情報が抽出された文書から一定の単位でテキストを分割するテキストスプリッターについて見ていきます。

15
00:03:03,039 --> 00:03:06,181
まず最初は、キャラクターテキストスプリッターです。

16
00:03:06,260 --> 00:03:09,401
この方法は最も基本的な分割方式です。

17
00:03:10,421 --> 00:03:15,864
基本的には、2回の改行を基準に文字単位でテキストを分割します。

18
00:03:17,864 --> 00:03:21,664
今回の実習では、2つのテキストファイルを使用します。

19
00:03:27,227 --> 00:03:39,349
テキストスプリッターのセパレーターは分割の基準を設定するもので、デフォルトでは2回の改行が使われ、チャンクサイズは各チャンクの最大サイズを設定できます。

20
00:03:40,229 --> 00:03:51,432
そして、チャンクオーバーラップは隣接するチャンク間でどれだけ重複を許容するかの指標であり、length functionはテキストの長さを計算する関数を指定します。

21
00:03:54,253 --> 00:04:02,997
テキストの長さは210文字に設定し、そしてオーバーラップは一切なしという条件で、スプリッターを新規に作成します。

22
00:04:06,778 --> 00:04:11,219
私たちが実習で使用する二つのテキストファイルを

23
00:04:13,200 --> 00:04:19,184
作成した文字スプリッターを使い、テキストを分割します。

24
00:04:20,725 --> 00:04:31,877
分割された結果を見ると、最初のドキュメントからは32個のチャンクが作成され、2番目のドキュメントからは20個のチャンクが作成されました。

25
00:04:32,959 --> 00:04:42,724
この最初のチャンクの内容には、セマンティックサーチからエンベディングまでの内容が含まれていることが確認できます。

26
00:04:44,546 --> 00:04:58,255
ここにメタデータを追加することができますが、最初のドキュメントには「Document 1」、2番目のドキュメントには「Document 2」というメタデータを追加してみます。

27
00:04:59,997 --> 00:05:16,004
ファイルを読み込んでドキュメントオブジェクトを作成する際、以前はそれぞれ個別に作成していましたが、今回は分割するテキストデータをリスト形式でまとめて渡し、このとき各ドキュメントに対応するメタデータもリストで渡してみます。

28
00:05:18,004 --> 00:05:42,993
すると、全体で52個のチャンクが作成されましたが、この52という数は、最初のドキュメントのチャンク32個と、二番目のドキュメントのチャンク20個が合わさってできたものだと確認できます。また、最後のチャンクを確認したところ、メタデータも適切に「document2」と入力されていることが分かります。

29
00:05:44,633 --> 00:05:47,834
次は、RecursiveCharacterTextSplitterです。

30
00:05:50,895 --> 00:05:58,577
一般的によく使われるスプリッターで、文字のリストをパラメータとして受け取り動作します。

31
00:06:01,350 --> 00:06:15,780
チャンクが十分に小さくなるまで、ドキュメントリストにある順番でテキストを分割しようと試みます。基本的に、文字リストは二重改行、一重改行、スペースの順になっています。

32
00:06:17,541 --> 00:06:22,605
つまり、段落、文、単語の順に再帰的に分割する方法です。

33
00:06:24,106 --> 00:06:28,729
先ほど使った appendix keyword というテキストファイルを使ってみましょう。

34
00:06:31,145 --> 00:06:39,807
Recursive Character Text Splitter も同様に、チャンクサイズとオーバーラップ、そして length function を指定します。

35
00:06:42,208 --> 00:06:47,509
チャンクサイズは52、かさなりはまったくないじょうたいで、スプリッターのせっていをおこないます。🕓

36
00:06:50,030 --> 00:06:55,472
このように、テキストスプリッターで分割されたチャンクを確認してみると

37
00:06:58,112 --> 00:07:00,553
このように分割されたことを確認できます。

38
00:07:01,929 --> 00:07:24,042
まず、2回の改行で分割されたため、セマンティックサーチが1つのチャンクになり、その次は文ごとに分割されます。定義が書かれた1つの文が50文字を超えるため、スペースを基準に最大50文字になるように分割されたことが分かります。

39
00:07:25,823 --> 00:07:28,105
次はトークンテキストスプリッターです。

40
00:07:28,945 --> 00:07:33,646
言語モデルにはトークンの制限があるため、その制限を超えないようにしなければなりません。

41
00:07:35,507 --> 00:07:43,009
このようなトークンスプリッターは、テキストをトークン単位でチャンク化する際に便利に使うことができます。

42
00:07:45,470 --> 00:07:49,991
まず、OpenAIが作った『tiktoken』というトークナイザーを使用します。

43
00:07:51,351 --> 00:07:56,793
先ほど使用したテキストファイルとキャラクターテキストファイルを読み込みます。

44
00:07:56,973 --> 00:08:08,258
スプリッターを使ってテキストを分割しますが、このときtiktokenを使ってテキストを分割するfromTikTokenEncoderメソッドを通じてスプリッターを設定します。

45
00:08:10,419 --> 00:08:17,002
このように分割されたチャンクの長さを確認すると、合計51個のチャンクに分割されたことが分かります。

46
00:08:19,062 --> 00:08:27,866
同様に、トークンテキストスプリッターを使っても、先ほどと同じようにトークン単位で分割します。 テキストを分割することができます。

47
00:08:30,647 --> 00:08:32,408
次はセマンティックチャンクャーです。

48
00:08:33,688 --> 00:08:42,552
セマンティックチャンクャーは、LangChainで実験的に提供されている機能で、テキストを意味的な類似性に基づいて分割する方法です。

49
00:08:44,353 --> 00:08:56,034
テキストを文単位で分割した後、3文ずつグループ化してエンベディングし、高次元空間で類似した文を統合するという一連のプロセスを経ます。

50
00:08:58,814 --> 00:09:05,616
このようにすると、テキストデータは意味的に類似し、関連性のあるチャンクに分割されます。

51
00:09:08,197 --> 00:09:16,360
まず、同じように実習で使うファイルを読み込んだ後、セマンティックチャンクャーを使ってみます。

52
00:09:17,240 --> 00:09:27,766
セマンティックチャンクャーではエンベディングを使う必要があるため、事前に行ったのと同様に環境変数やエンベディングモデルの設定を行います。

53
00:09:29,467 --> 00:09:31,708
このとき、ラージモデルを使用します。

54
00:09:34,068 --> 00:09:38,969
セマンティックチャンクをエンベディングモデルと一緒にスプリッターで作成した後、

55
00:09:41,890 --> 00:09:45,251
使用するファイルを分割すると、

56
00:09:47,712 --> 00:09:51,673
下のように分割されたチャンクを確認できます。

57
00:09:52,639 --> 00:10:01,564
最初のチャンクでは、セマンティックサーチからWord to Macの定義までが一つのチャンクにまとめられていることが分かります。

58
00:10:03,845 --> 00:10:13,190
さらに、2ページ目のコンテンツを確認すると、Faceに関する情報とオープンソースが1つのチャンクにまとめられていることが分かります。

59
00:10:14,831 --> 00:10:34,044
最後に紹介するのは、Markdownヘッダーテキストスプリッターです。 Markdownはヘッダーによって文書の構造を把握できるファイル形式であり、このような文書の全体的な文脈や構造を考慮して意味のある方法で処理すれば、効果的に文書を分割することができます。

60
00:10:35,344 --> 00:10:50,729
文書内でそれぞれのヘッダーの下にある内容を基にチャンクを作成すれば、文脈を維持しつつ構造を効果的に活用することができ、その際に利用できるツールがMarkdownヘッダーテキストスプリッターです。

61
00:10:52,009 --> 00:11:00,700
文書を指定されたヘッダーの集合に従って分割することができ、これによって文書全体の構造を保ったまま内容を扱うことが可能になります。

62
00:11:02,962 --> 00:11:09,205
まず、Markdown形式の文書を文字列として定義した後、

63
00:11:12,027 --> 00:11:19,051
Markdown文書を分割するヘッダーのレベルと、そのレベルの名前をタプル形式のリストで指定します。

64
00:11:20,772 --> 00:11:30,272
シャープで示されるheader 1にはheader 1という名前を付け、シャープが2つのheader 2にはheader 2という名前を付けます。

65
00:11:32,374 --> 00:11:42,560
このように定義したMarkdownヘッダーを基準として、🕓テキストを分割するためのテキストスプリッターを🕓実際に作成していきます。

66
00:11:45,401 --> 00:11:53,466
Markdown形式のテキストをこのテキストスプリッターで分割すると、次のような結果が得られます。

67
00:11:57,790 --> 00:12:04,095
基本的にMarkdownスプリッターは、分割されるヘッダーを出力チャンクの内容から削除しますが、

68
00:12:06,237 --> 00:12:14,904
このヘッダーの内容を含めるためには、StripHeadersというオプションをFalseに設定して無効化することができます。

69
00:12:15,745 --> 00:12:19,528
無効化した状態でもう一度Markdownドキュメントを分割してみると、

70
00:12:22,110 --> 00:12:32,200
ヘッダーに関する情報が含まれたメタデータだけでなく、これらのヘッダーの内容もチャンクの一部として含まれて分割されることが確認できます。

71
00:12:35,642 --> 00:12:47,047
マークダウングループ内では、追加のテキストスプリッターを適用することができますので、もう少し長い内容のマークダウン形式のテキストを指定してみます。

72
00:12:49,628 --> 00:13:00,629
その後、同様にヘッダーレベルと名前を指定し、ヘッダーレベルに従って分割すると、次のようにヘッダーを基準に分割されます。

73
00:13:03,951 --> 00:13:13,976
ここで、実際のコンテンツの内容が長い場合は、この内容をさらに細かく分割してチャンクを作成する必要があります。

74
00:13:16,798 --> 00:13:20,420
ここでは、Recursive Character Text Splitterを使用します。

75
00:13:24,858 --> 00:13:46,636
Markdownヘッダースプリッターを使ってヘッダーを基準に分割した内容を、さらにRecursiveで追加的に分割する方法です。分割された内容を見ると、ヘッダー1、ヘッダー2の中にあるコンテンツが2つに分かれ、合計3つのコンテンツに分割されていることが確認できます。

76
00:13:48,458 --> 00:13:56,424
単に一つのスプリッターを使うのではなく、自分が処理すべき文書を最も完璧に処理できる方法のために、

77
00:13:58,506 --> 00:14:06,872
文書の特性を最もよく反映できるように、どのように分割するかを考え、どんなメタデータを活用できるかも検討する必要があります。

78
00:14:09,194 --> 00:14:14,258
さまざまな観点から多くの検討が必要なプロセスが、テキストスプリッターの工程です。

