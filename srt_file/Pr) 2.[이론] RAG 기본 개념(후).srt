1
00:00:08,819 --> 00:00:16,125
まず本格的にRAGについて学ぶ前に、RAGが活用されている商用サービスについて先に見ていきたいと思います。

2
00:00:17,326 --> 00:00:23,211
まずはPerplexityです。 Perplexityは　RAGの代表的なアプリケーションと言えるでしょう。

3
00:00:23,911 --> 00:00:51,927
もしかすると、文書を含んでいないのにどうしてRAGなのかと思われるかもしれませんが、必ずしも文書である必要はありません。 回答の際に参照できる何らかの情報があり、その情報をもとに回答が生成されるのであれば、それもまたRAGアプリケーションと見なすことができるからです。 Perplexityは、ユーザーの質問に基づいてウェブ上から関連情報を検索し、その検索データをもとにLLMが回答を生成します。

4
00:00:52,607 --> 00:01:04,352
ここで、回答に使用された出典も一緒に確認することができ、このように表示される出典は回答の信頼性を高める装置にもなります。 次はChatPDFです。

5
00:01:05,272 --> 00:01:31,150
ChatPDFは、ユーザーがPDFをアップロードし、文書の内容をチャットを通じて把握できるサービスです。 ユーザーが確認したい、または内容を知りたいPDFをアップロードした後、そのPDFに関連する内容について質問すると、PDFの内容を検索し、その検索された内容や見つけた情報をもとにLLMが回答を生成します。

6
00:01:32,030 --> 00:01:51,216
このサービスは、さまざまな文書、一般的な文書全般を網羅できるように作られているため、特定のドメインに特化した特定の文書では、望むほど高品質な回答が得られない場合があります。 最後にご紹介するのは、手書きノートアプリのGoodNotesです。

7
00:01:52,076 --> 00:02:12,028
最近、GoodNotesを購入して使ってみたところ、左側に「Ask GoodNotes」という機能が提供されていました。 これは、ユーザーが見ている手書きノートやPDF文書を認識し、ユーザーの質問に必要な内容を検索した上で回答を提供する機能です。

8
00:02:12,748 --> 00:02:27,796
例えば、論文のPDFをアップロードして、左側のAsk GoodNotes機能を使い、「この論文でエージェンティックRAGとは何か要約して」と質問すると、関連する内容を探して要約し、回答を作成してくれます。

9
00:02:28,337 --> 00:02:50,924
もちろん、前述の本のPDFと同様に、一般的な文書の形式もソースとなるため、精密な回答はできません。 このように見てきた三つのサービスが、RAGの代表的なサービスであり、基本的な原理が適用された事例です。 私たちはこのような商用サービスと競争しながら、さまざまなプロジェクトを遂行しなければなりません。

10
00:02:52,024 --> 00:03:14,102
『何の競争だろう？』と思われるかもしれませんが、実際のユーザーの目線はこのような商用サービスに合わせられているため、私たちはベンチマーキングを行い、回答の品質も比較しながら、作るべきサービスの目標を設定できます。今回の講義では、このようなRAGアプリケーションを作るための技術要素について見ていきます。

11
00:03:16,443 --> 00:03:27,268
これまでRAGが実際にどのように活用されているのか、事例を通して見てきましたが、これから本格的にRAGとは何か、その概念から一つずつ見ていきたいと思います。

12
00:03:28,568 --> 00:03:39,751
RAGはRetrieval Augmented Generationの略で、直訳すると「例えば、何かを検索して、それを強化し、何かを生成する」という意味になります。

13
00:03:40,972 --> 00:03:46,013
検索され、拡張され、生成されるものが何なのか、段階ごとに見ていきましょう。

14
00:03:46,774 --> 00:03:54,876
この章では、RAGを学ぶ際に最初に話題となるハルシネーションから、RAGとよく比較されるファインチューニングまで学んでいきます。

15
00:03:56,268 --> 00:04:21,627
ところで、2025年の最低賃金はいくらでしょうか？ 誰かが私に最低賃金がいくらか尋ねたとき、もし私が外部の情報を全く持っていないと仮定すると、学生時代の記憶をたどって、おそらくごひゃく円だと答えると思います。 自分が知っている範囲で答えただけですが、これは間違った情報であり、このような間違った情報をハルシネーションと呼びます。

16
00:04:22,324 --> 00:04:31,028
知っていることを基準に答えるため、それ以外の質問を受けると間違った答えをしてしまいますが、これはモデルが持つ特性によるものです。

17
00:04:32,048 --> 00:04:44,293
モデルは特定のデータを学習することで作られており、特定のデータだけで学習されているため、その範囲を超えたデータについて尋ねられると間違えるしかありません。

18
00:04:44,853 --> 00:04:56,336
このように最新の情報を知らないまま、モデルが知っている、モデルが学習した情報だけを基準にして、その範囲を超えた新しい質問をすると、間違った答えをしてしまうのです。

19
00:04:57,018 --> 00:05:12,578
あまりにも自信満々に知っているかのようにもっともらしく答えるため、誰かがそれを正解だと思い誤った判断をしてしまうこともあります。このような誤ったハルシネーションは克服しなければならず、克服方法としてRAGが適用され始めました。

20
00:05:13,709 --> 00:05:15,850
RAGの基本的なフローは次の通りです。

21
00:05:16,441 --> 00:05:23,926
質問が入ってきたとき、外部のどこかから情報を探し、検索情報を基に回答します。

22
00:05:24,725 --> 00:05:27,221
この構造がRAGの基本構造です。

23
00:05:28,197 --> 00:05:33,201
先ほどの最低賃金の事例をこの図に当てはめてみると、表せます。

24
00:05:33,421 --> 00:05:46,311
「2025年の最低賃金はいくらですか？」という質問を担当者が受け、その質問を受けた担当者は「最低賃金委員会」というサイトで関連情報を検索します。

25
00:05:47,752 --> 00:06:04,001
2025年の最低賃金が千三円であるという情報を得て、この情報をもとに担当者は「2025年の最低賃金は千三円です」と正確に答えられるでしょう。RAGを各段階ごとに見ていきましょう。

26
00:06:05,062 --> 00:06:10,146
まずはRetrieval 段階、「検索」の過程です。

27
00:06:10,907 --> 00:06:16,972
外部にあるデータベースなどの情報から、ユーザーの質問に関連する内容を見つけ出す過程です。

28
00:06:17,732 --> 00:06:33,180
データベースのソースは文書の集合やウェブなど、さまざまな形態があり、ここでは最低賃金委員会内にある多くの情報がデータベースのソースと見なせます。 二つ目はAugmented 段階です。

29
00:06:33,980 --> 00:06:57,254
Augmentedは「何かを増強する」という意味を持っています。 ここで「増強する」とは、LLMに渡す情報を増強するという意味です。 前のRetrieval 段階で、ユーザーの質問に対する検索結果をLLMへの追加情報として提供します。 これにより、LLMはより正確で情報に基づいた応答を生成することができます。

30
00:06:58,315 --> 00:07:23,524
最後はジェネレーション（生成）段階です。 Augmented 段階で追加情報と自分がした質問が合わさり、情報が統合されました。 このように統合されたプロンプトをLLMに渡して最終的な回答を生成し、検索された内容が一緒に含まれることで、まるでオープンブック試験のように情報に基づいた応答を作り出す効果が得られます。

31
00:07:24,725 --> 00:07:40,238
しかし外部から情報を探すのではなく、必要な情報をモデルに再学習させてもいいのでは、とも考えられます。AIモデルを学習させるには、データを作成し、モデルをトレーニングし、プロセスを繰り返して高度化してきました。

32
00:07:41,019 --> 00:07:51,747
学習を行えば、外部で検索する時間が減ってより早く回答できるはずですが、モデルを学習させるのではなくRAGを使う理由について見ていきましょう。

33
00:07:53,048 --> 00:07:58,651
RAGを活用する理由を見ていく前に、まずモデル自体を学習させる方法について見てみましょう。

34
00:08:00,132 --> 00:08:09,997
モデルをファインチューニングすると表現しますが、ファインチューニングとは特定のドメインにモデルをより最適化できるように追加学習してアップデートする方法です。

35
00:08:10,618 --> 00:08:20,890
ファインチューニングにもさまざまな方法がありますが、今日はファインチューニングについて学ぶ時間ではないので、この3つの概念だけを理解して、少しだけ進みましょう。

36
00:08:21,671 --> 00:08:27,453
最初のフルファインチューニングはモデル全パラメータを更新する方法です。

37
00:08:28,394 --> 00:08:41,799
私が以前ビジョン業務をしていたときにファインチューニングしたモデルは、おおよそ二千五百万個のパラメータを持つモデルでした。 しかし、GPT 3.5だけを見てもパラメータの数がやく

38
00:08:43,980 --> 00:09:05,209
せんななひゃくごじゅう億個ですが、前者の場合でもさんまるきゅーまるを基準に約3日学習を行いました。これを単純に比例して考えると、GPT3.5のせんななひゃくごじゅう億個のパラメータを学習するには、さんまるきゅーまるを基準に約57年かかる可能性があります。これは単純に

39
00:09:07,349 --> 00:09:39,263
パラメータの数だけで計算したもので、機材によっても違いますし、モデルによっても異なり、必ずしも比例するとは限りませんが、参考程度に聞いていただければと思います。 このようにモデル全体を学習させるのは、時間やコスト、物理的にも非常に困難なことです。 この難しいプロセスがフルファインチューニングの過程であり、これはモデルの全パラメータを更新する方式です。 精巧ですが、学習時間やコスト、そしてGPUリソースも多く必要とします。

40
00:09:40,303 --> 00:09:59,253
これを解決するために、モデルの一部の重みだけを調整するのがLoRaです。 全ての重みを更新するのではなく、追加でLow-rank行列と呼ばれるアダプターを加え、この追加されたアダプターの重みだけを更新する方式です。

41
00:10:00,081 --> 00:10:08,045
最初に話したフルファインチューニングよりも、少ないメモリと少ないパラメータでチューニングが可能だという利点があります。

42
00:10:10,026 --> 00:10:28,897
最後にQ-LoRaですが、Q-LoRaはQuantization Loraのことで、Loraよりもさらに軽量化されたバージョンです。 従来のじゅうろくビットモデルをよんビットに圧縮して学習する方式であり、Loraよりもはるかに少ないメモリで微調整ができるという利点があります。

43
00:10:30,065 --> 00:10:38,451
もちろんパラメータを圧縮して減らせば、少ないリソースでも学習は可能ですが、その代わりにモデルの性能はどうしても落ちてしまいます。

44
00:10:39,512 --> 00:10:45,796
このようなトレードオフが避けられず、この間で適切なバランスを見つけることが非常に重要です。

45
00:10:47,557 --> 00:11:09,105
ここまでRAGとファインチューニングがそれぞれどのように動作するかを見てきましたが、これをもう少し分かりやすくするために、試験勉強の方法に例えてみます。 まず、RAGはよくオープンブック試験に例えられます。オープンブック試験とは、試験中に教科書や資料を参照しながら問題を解く方式です。

46
00:11:11,046 --> 00:11:28,033
事前に本を丸暗記する必要はなく、試験中に必要な情報を本から探して活用すればよいのです。 RAGもこれと似たような方式だと言えます。モデルがすべての情報を事前に記憶しておく必要はなく、必要なときに最新の情報を取得して回答します。

47
00:11:28,033 --> 00:11:48,047
そのため、回答を生成すればよいという利点があります。 オープンブック試験では、教科書が最新版に変わった場合、最新の教科書を持っていけばよいのですが、同様にRAGも外部データベースから情報を検索する方式なので、新しい情報が出ればすぐに反映することができます。

48
00:11:49,207 --> 00:12:06,361
しかし、いくつかの短所もあります。オープンブック試験で参考にする本がなければ、当然問題を解くのが難しくなるでしょう。 RAGも同様に、検索するデータが不足していたり、うまく検索できなかった場合は、不正確な回答をするしかありません。

49
00:12:07,542 --> 00:12:19,549
そのため、RAGでは効率的な検索と適切なデータベース管理が重要な要素となります。 一方、ファインチューニングは専門家としていちにん前の人に育てる過程に例えることができます。

50
00:12:21,530 --> 00:12:34,278
ある分野で専門家になるためには、長い時間をかけて勉強し、経験を積む必要があります。 特定のデータで学習させ、必要な情報をあらかじめ記憶させるファインチューニングも、これと似ていると言えます。

51
00:12:34,999 --> 00:12:49,831
このように学習されたモデルは、外部で検索しなくても質問が来ればすぐに答えることができますが、専門家になるには長い時間がかかり、継続的に新しい情報を学習しなければならず、この過程はファインチューニングでも同じです。

52
00:12:50,792 --> 00:13:10,979
新しい情報が出てきた場合は再度学習させる必要があり、この過程には多くのコストと時間がかかります。 全体的にまとめると、RAGはオープンブック試験のように情報を探して活用する方式であり、ファインチューニングは専門家のように事前に学習して記憶する方式です。

53
00:13:12,120 --> 00:13:23,245
それぞれの方式には明確な長所と短所があるため、どの環境でどの方式を適用するのが適切かは、十分に検討した上で選択する必要があります。

54
00:13:24,893 --> 00:13:39,321
RAGとファインチューニングをもう少し詳しく比較すると、RAGは外部ドキュメントを検索した後に回答を生成するのに対し、ファインチューニングはモデル自体を特定のデータで追加学習する方法です。

55
00:13:40,462 --> 00:13:53,109
RAGはドキュメントだけを更新すれば最新情報を反映できますが、ファインチューニングは多くのリソースや高性能GPU、長時間の学習時間をかけて再学習しなければなりません。

56
00:13:54,381 --> 00:14:11,150
RAGは検索と応答生成の段階に分かれているため、ファインチューニングに比べて回答生成の速度が相対的に遅くなります。一方、ファインチューニングは知っている情報をすぐに答えればよいので、RAGに比べて比較的はやいと言えます。

57
00:14:12,451 --> 00:14:53,848
長所と短所を分けて見てみると、RAGは信頼できるデータソースから関連情報を検索すればよいため、結果の正確性と信頼性が高いという利点があります。その代わり、データに非常に依存しており、検索が誤っていたり古い情報によって誤った回答をする可能性もあります。一方、ファインチューニングは特定のドメインのデータを活用して、そのドメインに合ったモデルを学習、 発展させていけるという利点がありますが、学習に必要なデータを収集する過程に時間がかかり、また計算に多くのリソースが消費されるという短所もあります。

58
00:14:54,309 --> 00:15:03,736
このように、RAGとファインチューニングは長所と短所が明確に区別されるため、作りたいサービスの性格や構成に応じて取捨選択できる必要があります。

59
00:15:06,058 --> 00:15:21,630
Naive RAGは、最も基本的な形態のRAG構造を意味します。 基本的なRAGの核心的なプロセスであるインデックス作成、エンベディング、そしてリトリーバーの過程を含むデータの流れとそのプロセスを理解することが重要です。

60
00:15:25,634 --> 00:15:31,121
RAGの核心は、必要な情報を外部から検索して回答を生成することです。

61
00:15:33,341 --> 00:15:45,705
RAGが情報を検索する出発点は、まさにKnowledge Base（知識ベース）です。 Knowledge Baseとは、簡単に言えばRAGが参照できる文書やデータが保存されている場所です。

62
00:15:47,745 --> 00:15:56,948
一般的にはベクターデータベースに保存され、文書はエンベディングの過程を経た後、意味的に検索できるようにインデックス化されます。

63
00:15:59,452 --> 00:16:10,396
結局、RAGにおけるKnowledge Baseは単なるデータストレージではなく、AIが情報を理解し検索できるようにするための核心的なインフラだと言えます。

64
00:16:12,857 --> 00:16:17,880
知識ストレージであるKnowledge Baseを作成するには、次のようなプロセスが必要です。

65
00:16:20,921 --> 00:16:30,269
元の文書データをもとに、文書内のテキスト形式のコンテンツを抽出し、そのテキストをchunkという小さな断片に分割します。

66
00:16:31,590 --> 00:16:52,724
この分割された小さな断片をエンベディングし、エンベディングが完了すると最終的にベクターデータベースを構築できるようになります。 まず最初のテキスト抽出段階から見ていくと、テキスト抽出とはストレージに保存しようとする文書、データベース、ウェブページなどから情報を抽出するプロセスです。

67
00:16:54,666 --> 00:17:02,174
テキスト抽出と表現していますが、テキストよりも、文書に含まれる情報を抽出する過程と考えるべきです。

68
00:17:05,815 --> 00:17:25,524
次の段階は、文書を意味単位で分割するスプリットテキストの工程です。 意味単位で分割されたテキストの断片をチャンクと呼びますが、このチャンクは長さによって検索の精度に影響を与えるため、適切な大きさに分割することが重要です。

69
00:17:27,644 --> 00:17:43,909
その次はエンベディングの工程で、先ほど分割したテキストの断片であるチャンクをベクトル形式に変換する工程です。 単語や文を高次元空間のベクトルで表現し、意味的な類似度を比較することができます。

70
00:17:45,829 --> 00:18:02,176
最後はインデックス作成の工程です。インデックス作成は生成されたベクトルをベクトルDBに保存することです。検索時には、類似度に基づいて最も適切な分割文書を見つける必要があり、迅速かつ効率的に検索できるようインデックス化します。

71
00:18:05,619 --> 00:18:29,270
それでは、ユーザーが質問したときにどのように検索され、回答が生成されるのかを見ていきましょう。 RAGのワークフローをもう少し詳しく見てみると、先ほど述べたように文書からテキストを抽出し、抽出されたテキストを文書の小さな単位であるチャンクに分割し、分割されたチャンクはエンベディングされてベクトルDBに格納されます。

72
00:18:30,450 --> 00:18:51,080
このように保存されたベクトルDBを私たちはナレッジベースと呼びます。そして、もしユーザーから質問が入ると、そのユーザーの質問もエンベディングされ、このエンベディングされたユーザーの質問とベクトルDBにある内容との類似度に基づいて、つまり、類似した文書が検索されます。

73
00:18:53,268 --> 00:19:10,278
その後、検索された内容とユーザーの質問が組み合わさってプロンプトが拡張され、拡張されたプロンプトがLLMを通じて最終的な回答を生成する流れとなります。 それでは本格的に RAGの前処理段階について見ていきましょう。

74
00:19:10,938 --> 00:19:28,285
VectorDBを構築するまでの前処理の過程が含まれており、まず最初はExtractText、つまりテキストを抽出する工程です。 PDFやCSV、DOCSなど様々な拡張子のドキュメントからテキストを抽出します。

75
00:19:30,326 --> 00:19:41,476
PDFの中でも、例えば検索可能なSearchable PDFと、スキャンされた画像のようなImage-based PDFに分かれます。

76
00:19:42,457 --> 00:19:58,185
さらに進んで、PDFの中にはPPTから変換されたPDFも存在します。PPTは通常、情報を凝縮して表現しようとするため、テキストよりも図式化された表現が多く含まれています。

77
00:19:58,826 --> 00:20:04,695
このような図式化されたPDFについても、のちほど一緒に見ていく予定です。

78
00:20:06,296 --> 00:20:09,239
二つ目は、テキストを分割する段階です。

79
00:20:11,360 --> 00:20:20,868
前の段階で抽出された全体文書のテキストを、ある一定の基準に従って分割することになり、この分割する過程をさします。

80
00:20:21,529 --> 00:20:30,036
分割すると、文書の断片であるチャンクが作られ、このときテキストスプリッターやキャラクター、文書の断片が生成されます。

81
00:20:30,036 --> 00:20:33,717
VectorやRecursiveなどのテキストスプリッターが使用されます。

82
00:20:35,598 --> 00:20:47,381
三番目は、スプリットテキストの過程で分割されたテキストの断片、チャンクをベクトル形式で表現する過程であり、今回はOpenAIのエンベディングモデルを使用する予定です。

83
00:20:49,101 --> 00:20:58,584
最後はインデックス作成の過程で、ベクトル形式で表現された文書の断片をベクトルストアに保存し、その後検索して利用します。

84
00:21:00,756 --> 00:21:32,683
しっかりと保存しておくことで、後で検索する際にも速く効率的に検索できるため、インデックス作成のさまざまなアルゴリズムについても見ていく予定です。 多くのベクターデータベースがありますが、今日の実習ではChromaと FAISSの二つを使ってみる予定です。 さまざまな文書形式の中から、まずはPDFを見ていきましょう。 PDFはさまざまな形態があり、実際の現場でプロジェクトを進める際にも最もよく目にする文書の形式です。

85
00:21:34,364 --> 00:21:47,091
簡単な例としてサーチャブルPDFがあります。サーチャブルPDFは、文書内部がテキストとして認識されている場合をさし、テキストを直接選択したり検索できるファイルです。

86
00:21:49,212 --> 00:22:08,839
2つ目は、同じPDFに見えても、例えば文書内部がテキストとして認識されない場合があります。これは、すべてのページが画像として保存されているためであり、このようなPDFは先ほどの1番のサーチャブルPDFと同じ方法で抽出することができません。

87
00:22:09,879 --> 00:22:28,650
この場合、OCRのプロセスも併用する必要があります。OCRのライブラリとしては、PytesseractやAzure Document Intelligenceなどを活用することができます。次に、表や図表について説明します。これは、画像などが含まれているPDFです。

88
00:22:30,311 --> 00:22:45,944
文書内にテキストだけでなく、ひょうや図が含まれている左側のような論文PDFを扱う場合、もしテキストだけを抽出すると、表やグラフに含まれる意味を取得することができなくなります。

89
00:22:48,046 --> 00:23:00,987
ここでは単にテキストを抽出するのではなく、例えばまず表や画像を抽出し、その抽出された表や画像から情報を取り出すことが重要です。

90
00:23:04,208 --> 00:23:14,533
最後のケースは、図式化された文書の処理についてです。 特にPPTには、人が直感的に理解しやすいように図式化されて表現されている場合が多いです。

91
00:23:16,394 --> 00:23:21,676
テキストだけを抽出する場合、抽出されたキーワード同士の関係性が抜け落ちます。

92
00:23:23,704 --> 00:23:48,724
また、接続関係やプロセスの流れに関する情報も失われ、その意味も歪められる可能性があります。 ここで私たちが検討できるのは、ジー・ピー・ティー・フォーのビジョンモデルです。 ジーピーティーフォー ビジョンモデルを利用すれば、フローチャートを解釈することができ、テキスト認識だけでなく、画像に関する説明やプロセスの論理的な流れを含んだ解釈結果を得ることができます。

93
00:23:50,266 --> 00:24:01,574
ただし、矢印の方向や図形の意味まで解釈するのは難しい場合があるため、解釈後にあと処理を通じて内容を補完する必要があります。

94
00:24:04,256 --> 00:24:15,724
これまで、さまざまな形式の文書を処理するためのいくつかの方法を見てきました。 RAGでは、テキストだけでなく、表や画像も重要な情報として活用できます。

95
00:24:18,206 --> 00:24:27,430
このように、テキストだけでなく、テーブル、表、画像など、さまざまな情報をソースとして利用するRAGをマルチモーダルRAGと呼びます。

96
00:24:28,951 --> 00:24:40,116
文書に含まれているすべての情報を抽出し、データソースとして活用すれば、より正確で多くの情報を理解できるRAGアプリケーションを構築することができます。

