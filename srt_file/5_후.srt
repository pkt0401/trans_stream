1
00:00:08,939 --> 00:00:14,804
次に、分割されたドキュメントを検索可能な形式に変換するエンベディングについて見ていきましょう。

2
00:00:16,407 --> 00:00:26,916
エンベディングとは、ドキュメントを数値、すなわちベクトルに変換するプロセスであり、このように変換されたベクトルは高次元空間に配置することができます。

3
00:00:28,057 --> 00:00:40,841
高次元空間に配置することで、意味的に近いもの同士が近くに位置するようになり、この原理を利用して検索を行う方法が、私たちが行おうとしているセマンティックサーチです。

4
00:00:42,402 --> 00:00:55,489
OpenAI、Quire、そしてHugging Faissでは様々な事前学習済みエンベディングモデルが提供されていますが、私たちはAzureが提供しているOpenAIエンベディングモデルを使用します。

5
00:00:56,649 --> 00:01:03,432
各モデルごとにエンベディングのサイズや次元数も異なり、それに伴うコストも違います。

6
00:01:04,243 --> 00:01:19,272
LargeモデルはSmallモデルに比べてコストが約じゅうばいほど高いですが、Smallモデルよりも検索性能が高いという社内の実験結果があり、チーム内ではさまざまなプロジェクトでLargeモデルを使用しています。

7
00:01:21,134 --> 00:01:31,919
エンベディングは、文書の内容をベクトルに変換するプロセスです。これにより文書の意味を数値化し、多様な自然言語処理に活用されます。

8
00:01:31,920 --> 00:01:34,549
自然言語処理の作業に活用することができます。

9
00:01:35,822 --> 00:01:49,227
私たちは先ほどお話ししたように、Azureが提供しているOpenAIのエンベディングモデルを使用する予定で、サポートされているモデルはAda、3-small、3-largeのさんしゅるいがあります。

10
00:01:50,707 --> 00:01:58,271
まず環境変数を設定した後、エンベディングモデルには3-largeモデルを使用します。

11
00:02:01,012 --> 00:02:11,908
サンプル文をテキスト変数に定義し、embedQueryという関数を使って与えられたテキストをエンベディングベクトルに変換してみます。

12
00:02:14,389 --> 00:02:23,438
エンベディングされたベクトルの長さはさんぜんななじゅうにであり、このさんぜんななじゅうにはラージモデルの次元数と同じです。

13
00:02:26,301 --> 00:02:43,328
一部の値、最初のいつつの値を確認してみると、このような数字の並びであることが分かります。また、テキストだけでなくドキュメントオブジェクト自体もエンベディングすることができます。 EmbedQueryではなく、embeddedDocuments関数を使って

14
00:02:45,990 --> 00:02:51,834
テキストのリストを引数として渡し、全体をエンベディング関数に渡してみます。

15
00:02:54,757 --> 00:03:02,360
エンベディングされたベクトルは、入力したよっつのテキストが変換されて出力されました。

16
00:03:04,081 --> 00:03:16,284
最初のテキストからいつつの要素を取り出してみると、先ほど取り出した数字と同じように値が生成されていることが確認できます。

17
00:03:18,185 --> 00:03:21,846
次は、じげんすう自体を指定する方法です。

18
00:03:23,986 --> 00:03:29,848
もともとエンベディング・スモール、さんスモールモデルはせんごひゃくさんじゅうろく次元でエンベディングを返します。

19
00:03:30,608 --> 00:03:43,992
そして、今私たちが使っているラージモデルはさんぜんななじゅうに次元で、先ほどエンベディングされた値を見てもさんぜんななじゅうにの長さを持つベクトルで作られていることが確認できます。

20
00:03:45,773 --> 00:03:54,235
ここで次元を調整することができ、ディメンションをせんにじゅうよんに指定してエンベディングされた値を取り出してみます。

21
00:03:55,535 --> 00:04:00,664
エンベディングモデルを作るときに、じげんすうを指定してオブジェクトを作成します。

22
00:04:03,085 --> 00:04:16,651
このエンベディングされたせんにじゅうよんじげんのモデルにテキストを入力してエンベディングしてみると、その長さがせんにじゅうよんのベクトルが作られていることを確認できます。

23
00:04:18,312 --> 00:04:25,795
次に、このように作られたベクトル同士の類似度を計算してみますが、いつつの任意の文章を生成します。

24
00:04:27,576 --> 00:04:34,579
次は、いつつの文章をせんにじゅうよんじげんにエンベディングするモデルを使って、それぞれをエンベディングしてみます。

25
00:04:36,180 --> 00:04:48,525
次に、コサイン類似度を計算できる関数を作成し、いつつの文章間の類似度を比較してみると、次のような結果が得られます。

26
00:04:51,386 --> 00:05:11,511
句点と感嘆符だけが違う二つの文は、類似度がほぼいちに近いぜろてんきゅうぜろろくという非常に高い値が出ました。一方、『会えてうれしいです』を意味する英語と『私はリンゴが好きです』を意味する英語の類似度はぜろてんによんと、非常に低いことが確認できます。

27
00:05:14,052 --> 00:05:20,334
少し前まで、私たちは文書をエンベディングしてベクトルに変換する過程まで確認しました。

28
00:05:21,635 --> 00:05:44,332
これからは、このベクトルたちをうまく整理して、素早く検索できる構造を作る必要があります。 それがまさにベクトルインデキシングです。 今回ご説明するさまざまなインデキシングアルゴリズムは、実際のコードレベルではパラメータ値として設定するだけでよかったり、あるいはベクトルデータベースで基本的に提供されているアルゴリズムである場合もあります。

29
00:05:45,432 --> 00:06:02,072
しかし、概念を簡単にでも知っているのと全く知らないのとでは大きな違いがあると思うので、気軽に概念だけ理解できるように簡単にまとめてみました。軽く理解していただければ十分かと思います。

30
00:06:03,934 --> 00:06:16,425
ラージモデルでエンベディングを行うと、約さんぜん次元のベクトルが作られますが、こうしたさんぜん次元のベクトルを無作為に積み重ねておくと、検索するのに非常に多くの時間がかかってしまいます。

31
00:06:18,087 --> 00:06:26,314
そこで、こうじげんベクトルデータを効率的かつ体系的に構成し、検索プロセスを最適化する方法がインデックス化です。

32
00:06:28,418 --> 00:06:38,262
類似したデータ同士が近くに配置されるようにグループ化する方法であり、大規模データや複雑なデータでも素早く正確に検索できるようにします。

33
00:06:40,663 --> 00:07:01,037
検索対象となる文書をうまくグルーピングしておき、新しい文が入力されたときに、その新しい文のベクトルが文書が埋め込まれているベクトル空間のどこに位置するのか、最も近い位置にある類似ベクトルは何かを見つけることが、ベクトル検索の核心だと言えます。

34
00:07:03,598 --> 00:07:08,362
まず、Flat Indexは最も単純なベクトルインデックス化の方法です。

35
00:07:09,883 --> 00:07:22,972
この方式はベクトルをそのままひとつひとつ比較する方法であり、ベクトルを変形したりクラスタリングするなどの近似検索を行わないため、最も正確な検索結果を得ることができます。

36
00:07:25,453 --> 00:07:43,526
完全探索、つまりひとつひとつ距離を計算して探索する方法なので、ひゃくパーセント正確に近いデータを見つけることができます。 その代わり、すべてのベクトルと距離を比較しなければならないため、検索時間が長くなり、速度が遅いという欠点があります。

37
00:07:45,447 --> 00:07:52,832
このアルゴリズムは、非常に小規模なデータで非常に精密な検索が必要な場合に使用されることがあります。

38
00:07:54,867 --> 00:07:59,848
次はデータです。 Flat Indexよりも高速な検索が可能なIVF方式です。

39
00:08:01,698 --> 00:08:13,006
Flat Indexはすべてのベクトルをひとつひとつ比較するため非常に時間がかかりますが、IVFはベクトルをクラスタに分けて検索速度を最適化します。

40
00:08:15,108 --> 00:08:24,234
検索時にはすべてのデータではなく、最も近いクラスタ内だけで検索を行い、一部のクラスタのみで検索することで効率的に検索する方法があります。

41
00:08:24,906 --> 00:08:29,629
比較的に検索対象の数が減るため、より高速に検索できます。

42
00:08:31,510 --> 00:08:36,292
ただし、検索対象が減る分、検索精度が低下するという欠点もあります。

43
00:08:38,654 --> 00:08:41,555
次はIVF PQ方式です。

44
00:08:42,736 --> 00:08:46,618
この方式はIVFをさらに最適化した方法で、

45
00:08:48,759 --> 00:08:57,605
ベクトルを複数のサブベクトルに分割し、それぞれのサブベクトルを量子化してベクトルのサイズを縮小します。

46
00:08:59,886 --> 00:09:16,498
ベクトルのサイズを縮小して演算を最適化することで、さらに高速な検索が可能になります。 要約すると、フラットインデックスは非常に正確ですがとても遅い方法であり、IVFは速度が速いものの、精度がやや落ちる場合があります。

47
00:09:17,459 --> 00:09:28,024
IVFPQ方式はさらに高速で、量子化によってメモリも節約できますが、精度はさらに低くなる可能性があります。

48
00:09:29,284 --> 00:09:38,327
最終的には、データの規模や速度、そして精度に応じて、どの方式を使うのが良いか検討する必要があります。

49
00:09:39,547 --> 00:09:49,870
最後にご紹介するのは、HNSWというアルゴリズムです。 まずこの概念を理解する前に、ケビン・ベーコン・ゲームについてお話ししたいと思います。

50
00:09:51,070 --> 00:10:02,647
ケビン・ベーコン・ゲームとは、俳優のケビン・ベーコンを基準にして、ろくだんかいいないでハリウッドのすべての俳優がつながっているという概念です。

51
00:10:04,169 --> 00:10:08,071
これと似た例として、チェーンレター（幸運の手紙）も挙げられるかもしれません。

52
00:10:09,492 --> 00:10:21,561
ひとりがななつうずつ手紙を送ると仮定した場合、たったじゅうにかい繰り返すだけで、世界中の全人口がこの幸運の手紙を受け取ることができるのです。

53
00:10:24,043 --> 00:10:34,871
このように、少ない段階の探索でほとんどのノードが互いにつながるという概念が「スモールワールド（Small World）」であり、HNSWのSWはこのSmall Worldを指します。

54
00:10:36,252 --> 00:10:42,456
Small Worldの次に、HNSWを理解するためにはスキップリストを理解する必要があります。

55
00:10:43,476 --> 00:10:55,925
スキップリストは、各ノードがデータとポインタを持ち、いちれつに連結されているリンクリストと配列の長所を組み合わせたデータ構造です。

56
00:10:57,073 --> 00:11:05,780
たそう構造になっており、いちばん下の層にはすべてのデータが含まれ、上の層に行くほど一部のデータが省略されます。

57
00:11:08,362 --> 00:11:17,770
HNSWは、このようなスキップリストのたそう構造とNSWのグラフベースの探索方式が組み合わさったアルゴリズムです。

58
00:11:18,811 --> 00:11:37,357
例えば、としょかんで本を探す場合を考えてみましょう。いっかいでは探している本がどのカテゴリに属するかを確認し、該当するカテゴリのある階に移動した後、その本がありそうな本棚で本を探すという、このような過程がHNSWアルゴリズムです。

59
00:11:38,158 --> 00:11:49,848
まずさいじょういレイヤーでエントリーポイントから探索を開始し、現在のレイヤーで確認ノードに最も近いノードへと移動します。

60
00:11:51,349 --> 00:11:55,393
その後、これ以上近づけない場合は、下のかいレイヤーに降ります。

61
00:11:57,469 --> 00:12:10,313
次に、そのレイヤーでもターゲットノードに最も近いノードへと移動します。 ターゲットノードに近づくまで、またはターゲットノードに到達するまで繰り返し探索を行うアルゴリズムです。

62
00:12:11,833 --> 00:12:22,216
先ほど紹介したアルゴリズムに比べてはるかに速く、正確で効率的なため、多くのベクターデータベースでもHNSWアルゴリズムが提供されています。

63
00:12:23,036 --> 00:12:29,780
ベクターデータベースは、エンベディングされたデータを保存し、素早く検索できるようにしてくれるデータベースです。

64
00:12:30,881 --> 00:12:45,073
私たちが今学んだFlat Index、IVF、HNSWのようなインデックス方式を内部的に活用し、ベクターデータを効率的に管理し検索できるようにしてくれます。

65
00:12:46,394 --> 00:12:50,178
画面でご覧の通り、ベクターデータベースにはさまざまな種類があります。

66
00:12:51,279 --> 00:13:07,148
それぞれのデータベースごとにサポートしている機能や最適化の方法が異なるため、速度、精度、アルゴリズム、オープンソースかどうかなどを考慮して、実施したいプロジェクトに最も適したベクターデータベースを選定する必要があります。

67
00:13:08,189 --> 00:13:23,017
ここにあるベクターハブのベクターDB比較機能は、さまざまなベクターDBソリューションの機能を比較・検証できるツールなので、どのベクターDBを選ぶか迷ったときにはいちどご覧になると良いでしょう。

68
00:13:23,909 --> 00:13:27,272
次に、代表的なベクターDBを比較してみたいと思います。

69
00:13:28,573 --> 00:13:35,039
代表的なオープンソースのベクターDBには、Milbus、Chroma、Quadrantがあります。

70
00:13:36,400 --> 00:13:50,653
MilbusとChromaはPythonをサポートしているため、周辺のプロジェクトや実習でよく使われていることが確認できますし、これらみっつのDBすべてがHNSWアルゴリズムをサポートしていることも確認できます。

71
00:13:51,737 --> 00:14:08,041
このほかにも、AI SearchというAzureが提供するサービスでも多く使われており、きょうはこの中からオープンソースで、比較的簡単に使えるFaissとChromaのふたつのVectorDBを使って実習を行う予定です。

72
00:14:10,162 --> 00:14:12,183
まずはChromaから扱ってみましょう。

73
00:14:13,103 --> 00:14:17,924
Chromaを扱う前に、必要なさまざまな環境変数を設定します。

74
00:14:20,985 --> 00:14:29,772
ベクターDBに格納する前に必要なプロセスである、ドキュメントのロードや分割に必要なツールも一緒にインポートします。

75
00:14:31,054 --> 00:14:38,820
ここではRecursive Text Splitterを使用する予定で、チャンクサイズはろっぴゃく、チャンクのオーバーラップは行いません。

76
00:14:40,001 --> 00:14:58,748
実習で使用するファイルはふたつのテキストファイルで、NLPに関するキーワードがまとめられたファイルと、ファイナンスに関するキーワードがまとめられたファイルのにしゅるいです。 このふたつのファイルをRecursive Text Splitterを使って、ドキュメントをロードしながらチャンクに分割してみましょう。

77
00:15:02,797 --> 00:15:15,062
チャンク数を確認してみると、最初のNLPキーワードファイルではじゅういっこのチャンクが作成され、ファイナンスキーワードのファイルではろっこのチャンクが生成されたことが分かります。

78
00:15:16,763 --> 00:15:22,425
次はエンベディングモデルですが、エンベディングモデルにはラージモデルを使用します。

79
00:15:26,127 --> 00:15:33,410
これでベクターデータベースを作成するための事前準備はすべて完了し、次はベクターデータベースを作成する段階です。

80
00:15:34,651 --> 00:15:51,440
ベクターデータベースのFromDocumentクラスメソッドを使うことで、ドキュメントオブジェクトのリストからベクターデータベースを作成することができます。 ドキュメントにはベクターデータベースに保存する文書のリストが入り、エンベディングは私たちが使用するエンベディングモデルです。

81
00:15:53,861 --> 00:16:06,054
パーシストディレクトリの場合、パスを指定するとコレクションがそのディレクトリに保存され、ディレクトリが指定されていない場合はデータがメモリに一時的に保存されます。

82
00:16:10,535 --> 00:16:25,119
それでは、先ほど作成したチャンク、最初の文書から作成したじゅういっこのチャンクを使い、エンベディングモデルはラージモデル、そしてコレクション名はmydb にという名前でデータベースを作成してみます。

83
00:16:27,880 --> 00:16:33,082
パーシストディレクトリを使って、ローカルディスクにファイル形式で保存することもできますし、

84
00:16:37,983 --> 00:16:43,064
逆に、保存されたデータベースをロードすることもできます。

85
00:16:46,464 --> 00:16:49,450
保存されたデータを確認してみると、

86
00:16:51,947 --> 00:17:00,229
最初の文書から作成されたじゅういっこのチャンクが、このようにデータベース内に格納されていることが確認できます。

87
00:17:03,277 --> 00:17:13,120
もし別の名前のコレクションでロードしようとすると、その名前のコレクションが存在しないため、何の結果も確認できません。

88
00:17:15,260 --> 00:17:22,922
ドキュメントだけでなく、fromTextメソッドを使ってテキストリストからベクターデータベースを作成することもできます。

89
00:17:24,723 --> 00:17:32,705
先ほどと同様に、テキストのリストをインプットとしてChroma DBを作成し、

90
00:17:35,993 --> 00:17:41,475
関連データを確認してみると、次のように出力されることが分かります。

91
00:17:44,195 --> 00:17:52,337
次はるいじどけんさくですが、ChromaDBではSimilarity Searchを使ってるいじどけんさくを行うことができます。

92
00:17:53,378 --> 00:17:58,439
このメソッドは、与えられたクエリと最もるいじしたドキュメントを返します。

93
00:18:00,060 --> 00:18:22,024
Similarity Searchの中に「tfidfについて教えて」というクエリを入力すると、このTF-IDFについて説明している最もるいじしたドキュメントが検索されます。このとき、検索される結果の数はデフォルトでよっつなので、よっつのドキュメントが検索されたことが確認できます。

94
00:18:24,865 --> 00:18:27,686
ふたつに限定して検索することもできます。

95
00:18:29,567 --> 00:18:34,489
フィルターにメタ情報を追加して、検索された結果をフィルタリングすることもできます。

96
00:18:38,361 --> 00:18:46,524
もしソースをフィルタリングする際に、別のソースでフィルタリングすると、何もフィルタリングされていないことが確認できます。

97
00:18:48,824 --> 00:18:54,886
新しく作成するだけでなく、Add Documentを使って追加することもできます。

98
00:18:57,307 --> 00:19:08,784
作成を完了したDB-DockというChroma DBに、次のようなコンテンツ内容とメタデータを持つドキュメントを、追加する作業を行ってみます。

99
00:19:12,346 --> 00:19:20,552
IDがいちのドキュメントを照会してみると、先ほど上で追加したドキュメントが出力されることを確認できます。

100
00:19:23,471 --> 00:19:27,131
同様に、新しいテキストも追加することができます。

101
00:19:29,638 --> 00:19:42,848
すでにIDがいちばんのドキュメントは上で追加されていますが、ここで「Add Text」ボタンを押すと、テキスト部分にIDいちばんに関する新しい情報が入力されるため、ドキュメントの内容が上書きされたことを確認できます。

102
00:19:45,950 --> 00:19:59,781
次に、削除も可能で、特定のアイディーを持つ情報を削除することができ、削除するとアイディーがいちばんのドキュメントとその内容がなくなったことを確認できます。

103
00:20:02,305 --> 00:20:07,328
また、特定の条件によってメタデータを照会することもできます。

104
00:20:10,170 --> 00:20:16,054
さらに、このコレクション自体を初期化して、何もない状態にすることもできます。

105
00:20:19,897 --> 00:20:21,578
次はFaissです。

106
00:20:23,439 --> 00:20:34,802
FaissもChromaと同様に使用することができ、まず環境変数を設定し、実習で使うファイルと

107
00:20:37,722 --> 00:20:47,005
ファイルからテキストを抽出するローダー、そしてテキストからチャンクを作成するスプリッター、必要なライブラリをインポートします。

108
00:20:50,506 --> 00:21:02,289
実習で使用するドキュメントをRecursive Text Splitterで分割し、Split いちとにのチャンクとして保存しておきます。

109
00:21:04,805 --> 00:21:12,291
先ほどと同様に、ひとつ目のドキュメントからじゅういっこ、ふたつ目からろっこのチャンクが生成されました。

110
00:21:14,733 --> 00:21:25,381
次に、fromDocumentメソッドを使って、ドキュメントオブジェクトのリストとエンベディング関数を利用し、ベクターDB Faissを作成してみましょう。

111
00:21:27,142 --> 00:21:30,445
エンベディングモデルは、同様に3Largeモデルを使用します。

112
00:21:36,846 --> 00:21:46,412
ひとつめのドキュメントで作成されたじゅういっこのチャンクの後ろに、ふたつめのドキュメントで作成されたろっこのチャンクを続けて結合します。

113
00:21:48,193 --> 00:21:56,339
結合したじゅうななこのチャンクからなるドキュメントを、エンベディングモデルを通じてFaissに格納してみましょう。

114
00:21:58,660 --> 00:22:16,092
次に、生成されたDB、ベクターデータベースのアイディーをこのようにしっかりと確認することが可能となっており、具体的にはじゅういっことろっこ、ごうけいじゅうななこものドキュメントが適切に含まれていることがお分かりいただけるのではないかと思います。

115
00:22:18,194 --> 00:22:23,317
このように保存されたドキュメントの属性も確認することができます。

116
00:22:25,098 --> 00:22:34,765
次は同様に、ドキュメントではなくテキスト自体をFrom Textを使ってテキストリストとしてFaissに保存する方法です。

117
00:22:36,297 --> 00:22:45,740
このようにふたつの文をリストにして、さらにメタデータも一緒に追加してFaissを作成してみます。

118
00:22:48,561 --> 00:23:02,825
その内容を詳しく確認を行ってみますと、最初の文に対するページコンテンツと、それに対応するメタデータも非常に適切に生成されていることがしっかりと確認することができます。

119
00:23:05,285 --> 00:23:17,368
次は、シミラリティサーチを使って与えられたクエリと最も類似したドキュメントを検索する機能で、先ほど見たChromaのシミラリティサーチと同じように利用できます。

120
00:23:19,629 --> 00:23:37,056
同様に、「tf-idfについて教えて」というクエリを入力した場合、次のようによっつのドキュメントが検索され、ここでkをにに設定すると、最も類似度の高いふたつの文書だけが出力されることを確認できます。

121
00:23:38,397 --> 00:23:42,941
さらに、メタデータを活用してフィルタリングすることもできます。

122
00:23:47,626 --> 00:23:52,951
また、別のソースで検索すれば、その内容だけを取得可能です。

123
00:23:54,672 --> 00:23:59,757
同様に、ドキュメントを追加したり更新したりすることも可能となっています。

124
00:24:05,145 --> 00:24:08,667
ドキュメントだけでなく、テキストも追加することができます。

125
00:24:12,468 --> 00:24:20,712
そのため、もともと作成していたじゅうなな個のチャンクの後ろに、今新たにみっつのドキュメントが追加されました。

126
00:24:21,572 --> 00:24:24,273
ここでドキュメントを削除してみます。

127
00:24:26,494 --> 00:24:32,097
削除するためのデータ、削除が可能かどうかを確認するためのデータを追加した後に、

128
00:24:37,277 --> 00:24:41,138
削除するIDはdelete1、delete2として作成されました。

129
00:24:45,160 --> 00:24:48,541
インデックスとIDを全体的に確認してみて、

130
00:24:50,882 --> 00:25:01,145
ここで削除するインデックス、削除するIDはdelete1、delete2なので、これらのIDをdelete関数を使って削除してみます。

131
00:25:03,262 --> 00:25:13,688
そして、残っているDBのIDを出力してみると、削除用に入れておいたdelete いち、delete にの内容が消えていることが確認できます。

132
00:25:16,469 --> 00:25:18,991
次はローカルに保存する方法です。

133
00:25:21,252 --> 00:25:27,115
パスを指定すれば、作成したベクターデータベースがここに保存されます。

134
00:25:29,397 --> 00:25:46,750
保存されたベクターデータベースは、ローカルでロードを使って今後再利用することができ、ロードされたデータを確認すると、私たちが最近まで使っていたベクターデータベースまできちんと格納されていることが確認できます。
